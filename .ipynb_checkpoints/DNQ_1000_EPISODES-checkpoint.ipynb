{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84b045f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 09:25:26.772378: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-20 09:25:26.776392: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-20 09:25:26.789154: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734686726.810320  145438 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734686726.816627  145438 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-20 09:25:26.839159: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "\n",
    "!export TF_CPP_MIN_LOG_LEVEL=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e6e6a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "GAMMA = 0.99\n",
    "EPISODES = 1000\n",
    "BATCH_SIZE = 64\n",
    "MEMORY_SIZE = 10000\n",
    "UPDATE_FREQUENCY = 100\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f347759",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ee47f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(tf.keras.Model):\n",
    "    def __init__(self, action_space):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(24, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(24, activation='relu')\n",
    "        self.out = tf.keras.layers.Dense(action_space, activation='linear')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67c32535",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, model, action_space):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(range(action_space))\n",
    "    else:\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        q_values = model(state)\n",
    "        return np.argmax(q_values.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "476ab7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(model, target_model, memory, batch_size, gamma, optimizer):\n",
    "    if memory.size() < batch_size:\n",
    "        return\n",
    "\n",
    "    batch = memory.sample(batch_size)\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "    states = np.array(states)\n",
    "    next_states = np.array(next_states)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        q_values = model(states)\n",
    "        next_q_values = target_model(next_states)\n",
    "        \n",
    "        target_q_values = q_values.numpy()\n",
    "        for i in range(batch_size):\n",
    "            if dones[i]:\n",
    "                target_q_values[i][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                target_q_values[i][actions[i]] = rewards[i] + gamma * np.max(next_q_values[i])  # 非终止状态\n",
    "\n",
    "        target_q_values = tf.convert_to_tensor(target_q_values, dtype=tf.float32)\n",
    "        loss = tf.reduce_mean(tf.square(q_values - target_q_values))\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae7cdc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = env.action_space.n\n",
    "model = QNetwork(action_space)\n",
    "target_model = QNetwork(action_space)\n",
    "target_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a2e632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(MEMORY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ac9b4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 09:25:41.012092: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8224ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: 14.0, ε: 0.99, Traning time: 0.00 s\n",
      "Episode 10, Total Reward: 21.0, ε: 0.95, Traning time: 6.59 s\n",
      "Episode 20, Total Reward: 37.0, ε: 0.90, Traning time: 15.91 s\n",
      "Episode 30, Total Reward: 36.0, ε: 0.86, Traning time: 27.61 s\n",
      "Episode 40, Total Reward: 20.0, ε: 0.81, Traning time: 35.09 s\n",
      "Episode 50, Total Reward: 12.0, ε: 0.77, Traning time: 42.94 s\n",
      "Episode 60, Total Reward: 14.0, ε: 0.74, Traning time: 50.29 s\n",
      "Episode 70, Total Reward: 21.0, ε: 0.70, Traning time: 56.12 s\n",
      "Episode 80, Total Reward: 19.0, ε: 0.67, Traning time: 63.74 s\n",
      "Episode 90, Total Reward: 19.0, ε: 0.63, Traning time: 69.95 s\n",
      "Episode 100, Total Reward: 13.0, ε: 0.60, Traning time: 74.97 s\n",
      "Episode 110, Total Reward: 15.0, ε: 0.57, Traning time: 86.52 s\n",
      "Episode 120, Total Reward: 12.0, ε: 0.55, Traning time: 92.71 s\n",
      "Episode 130, Total Reward: 18.0, ε: 0.52, Traning time: 100.11 s\n",
      "Episode 140, Total Reward: 14.0, ε: 0.49, Traning time: 105.90 s\n",
      "Episode 150, Total Reward: 11.0, ε: 0.47, Traning time: 111.41 s\n",
      "Episode 160, Total Reward: 17.0, ε: 0.45, Traning time: 117.17 s\n",
      "Episode 170, Total Reward: 14.0, ε: 0.42, Traning time: 123.30 s\n",
      "Episode 180, Total Reward: 14.0, ε: 0.40, Traning time: 129.56 s\n",
      "Episode 190, Total Reward: 9.0, ε: 0.38, Traning time: 134.53 s\n",
      "Episode 200, Total Reward: 9.0, ε: 0.37, Traning time: 140.06 s\n",
      "Episode 210, Total Reward: 38.0, ε: 0.35, Traning time: 150.42 s\n",
      "Episode 220, Total Reward: 11.0, ε: 0.33, Traning time: 155.84 s\n",
      "Episode 230, Total Reward: 12.0, ε: 0.31, Traning time: 163.30 s\n",
      "Episode 240, Total Reward: 11.0, ε: 0.30, Traning time: 170.63 s\n",
      "Episode 250, Total Reward: 15.0, ε: 0.28, Traning time: 175.98 s\n",
      "Episode 260, Total Reward: 13.0, ε: 0.27, Traning time: 182.84 s\n",
      "Episode 270, Total Reward: 14.0, ε: 0.26, Traning time: 196.03 s\n",
      "Episode 280, Total Reward: 13.0, ε: 0.24, Traning time: 206.15 s\n",
      "Episode 290, Total Reward: 10.0, ε: 0.23, Traning time: 212.27 s\n",
      "Episode 300, Total Reward: 18.0, ε: 0.22, Traning time: 222.64 s\n",
      "Episode 310, Total Reward: 77.0, ε: 0.21, Traning time: 260.57 s\n",
      "Episode 320, Total Reward: 15.0, ε: 0.20, Traning time: 290.75 s\n",
      "Episode 330, Total Reward: 34.0, ε: 0.19, Traning time: 316.13 s\n",
      "Episode 340, Total Reward: 36.0, ε: 0.18, Traning time: 342.66 s\n",
      "Episode 350, Total Reward: 88.0, ε: 0.17, Traning time: 364.95 s\n",
      "Episode 360, Total Reward: 51.0, ε: 0.16, Traning time: 387.84 s\n",
      "Episode 370, Total Reward: 51.0, ε: 0.16, Traning time: 410.19 s\n",
      "Episode 380, Total Reward: 30.0, ε: 0.15, Traning time: 431.35 s\n",
      "Episode 390, Total Reward: 63.0, ε: 0.14, Traning time: 453.69 s\n",
      "Episode 400, Total Reward: 46.0, ε: 0.13, Traning time: 473.88 s\n",
      "Episode 410, Total Reward: 127.0, ε: 0.13, Traning time: 516.86 s\n",
      "Episode 420, Total Reward: 25.0, ε: 0.12, Traning time: 559.93 s\n",
      "Episode 430, Total Reward: 77.0, ε: 0.12, Traning time: 613.25 s\n",
      "Episode 440, Total Reward: 121.0, ε: 0.11, Traning time: 663.46 s\n",
      "Episode 450, Total Reward: 89.0, ε: 0.10, Traning time: 721.81 s\n",
      "Episode 460, Total Reward: 149.0, ε: 0.10, Traning time: 771.90 s\n",
      "Episode 470, Total Reward: 113.0, ε: 0.09, Traning time: 823.17 s\n",
      "Episode 480, Total Reward: 278.0, ε: 0.09, Traning time: 881.46 s\n",
      "Episode 490, Total Reward: 133.0, ε: 0.09, Traning time: 940.32 s\n",
      "Episode 500, Total Reward: 151.0, ε: 0.08, Traning time: 1013.54 s\n",
      "Episode 510, Total Reward: 159.0, ε: 0.08, Traning time: 1134.97 s\n",
      "Episode 520, Total Reward: 169.0, ε: 0.07, Traning time: 1206.36 s\n",
      "Episode 530, Total Reward: 47.0, ε: 0.07, Traning time: 1334.32 s\n",
      "Episode 540, Total Reward: 173.0, ε: 0.07, Traning time: 1497.33 s\n",
      "Episode 550, Total Reward: 155.0, ε: 0.06, Traning time: 1644.26 s\n",
      "Episode 560, Total Reward: 138.0, ε: 0.06, Traning time: 1715.47 s\n",
      "Episode 570, Total Reward: 134.0, ε: 0.06, Traning time: 1781.12 s\n",
      "Episode 580, Total Reward: 137.0, ε: 0.05, Traning time: 1900.73 s\n",
      "Episode 590, Total Reward: 145.0, ε: 0.05, Traning time: 2049.80 s\n",
      "Episode 600, Total Reward: 146.0, ε: 0.05, Traning time: 2192.86 s\n",
      "Episode 610, Total Reward: 156.0, ε: 0.05, Traning time: 2319.30 s\n",
      "Episode 620, Total Reward: 148.0, ε: 0.04, Traning time: 2469.54 s\n",
      "Episode 630, Total Reward: 130.0, ε: 0.04, Traning time: 2589.80 s\n",
      "Episode 640, Total Reward: 144.0, ε: 0.04, Traning time: 2712.39 s\n",
      "Episode 650, Total Reward: 134.0, ε: 0.04, Traning time: 2837.66 s\n",
      "Episode 660, Total Reward: 113.0, ε: 0.04, Traning time: 2956.70 s\n",
      "Episode 670, Total Reward: 135.0, ε: 0.03, Traning time: 3073.21 s\n",
      "Episode 680, Total Reward: 120.0, ε: 0.03, Traning time: 3186.43 s\n",
      "Episode 690, Total Reward: 140.0, ε: 0.03, Traning time: 3312.90 s\n",
      "Episode 700, Total Reward: 131.0, ε: 0.03, Traning time: 3426.89 s\n",
      "Episode 710, Total Reward: 139.0, ε: 0.03, Traning time: 3550.78 s\n",
      "Episode 720, Total Reward: 130.0, ε: 0.03, Traning time: 3675.31 s\n",
      "Episode 730, Total Reward: 132.0, ε: 0.03, Traning time: 3797.73 s\n",
      "Episode 740, Total Reward: 134.0, ε: 0.02, Traning time: 3915.54 s\n",
      "Episode 750, Total Reward: 123.0, ε: 0.02, Traning time: 4034.03 s\n",
      "Episode 760, Total Reward: 129.0, ε: 0.02, Traning time: 4150.13 s\n",
      "Episode 770, Total Reward: 124.0, ε: 0.02, Traning time: 4268.88 s\n",
      "Episode 780, Total Reward: 122.0, ε: 0.02, Traning time: 4381.41 s\n",
      "Episode 790, Total Reward: 124.0, ε: 0.02, Traning time: 4469.74 s\n",
      "Episode 800, Total Reward: 122.0, ε: 0.02, Traning time: 4525.19 s\n",
      "Episode 810, Total Reward: 128.0, ε: 0.02, Traning time: 4582.20 s\n",
      "Episode 820, Total Reward: 131.0, ε: 0.02, Traning time: 4641.69 s\n",
      "Episode 830, Total Reward: 135.0, ε: 0.02, Traning time: 4704.24 s\n",
      "Episode 840, Total Reward: 135.0, ε: 0.01, Traning time: 4767.26 s\n",
      "Episode 850, Total Reward: 124.0, ε: 0.01, Traning time: 4833.01 s\n",
      "Episode 860, Total Reward: 132.0, ε: 0.01, Traning time: 4896.69 s\n",
      "Episode 870, Total Reward: 228.0, ε: 0.01, Traning time: 4975.42 s\n",
      "Episode 880, Total Reward: 149.0, ε: 0.01, Traning time: 5064.45 s\n",
      "Episode 890, Total Reward: 188.0, ε: 0.01, Traning time: 5147.25 s\n",
      "Episode 900, Total Reward: 183.0, ε: 0.01, Traning time: 5233.00 s\n",
      "Episode 910, Total Reward: 172.0, ε: 0.01, Traning time: 5330.23 s\n",
      "Episode 920, Total Reward: 202.0, ε: 0.01, Traning time: 5434.50 s\n",
      "Episode 930, Total Reward: 246.0, ε: 0.01, Traning time: 5538.04 s\n",
      "Episode 940, Total Reward: 211.0, ε: 0.01, Traning time: 5679.28 s\n"
     ]
    }
   ],
   "source": [
    "reward_list = []\n",
    "\n",
    "start_time = time.time()\n",
    "for episode in range(EPISODES):\n",
    "    state = env.reset()[0]\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = epsilon_greedy_policy(state, epsilon, model, action_space)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        replay_buffer.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        train_dqn(model, target_model, replay_buffer, BATCH_SIZE, GAMMA, optimizer)\n",
    "\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    if episode % UPDATE_FREQUENCY == 0:\n",
    "        target_model.set_weights(model.get_weights())\n",
    "\n",
    "    reward_list.append(total_reward)\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}, ε: {epsilon:.2f}, Traning time: {elapsed_time:.2f} s\")\n",
    "        \n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Total training time: {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1416127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_list)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Total Reward during DQN Training (Maximum Reward: 500)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
